{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple learning network in MLJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boot up MLJ and load some demonstration data (the Boston dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using DataFrames\n",
    "using Statistics\n",
    "\n",
    "Xraw, yraw = datanow();\n",
    "train, test = partition(eachindex(yraw), 0.7); # 70:30 split\n",
    "Xtrain, ytrain = Xraw[train,:], yraw[train];\n",
    "Xtest, ytest = Xraw[test,:], yraw[test];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every network needs source nodes where data enters the network. The data to be used for training is placed at the source node (at its `data` field, which is mutable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mX\u001b[22m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@constant X = node(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1my\u001b[22m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@constant y = node(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Using the `@constant` macro is equivalent to making a `const` declaration but with the name of the bound variable registered for REPL display.)\n",
    "\n",
    "We want to fit a K-nearest neighbor to our data, but with the input data standardized. That is, we want to rescale all inputs so the columns in the training data have zero mean and unit standard deviation. \n",
    "\n",
    "The standardization of a `DataFrame` is described by a single hyperparameter, namely a list of  features (column names) to be standardized. The default is an empty list, indicating that all numerical features should be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mStandardizer @ 5…72\u001b[22m: \n",
       "features                =>   0-element Array{Symbol,1}\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_model = Standardizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our rescaling transformation will look to `X` for its training data, we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainable(\u001b[0m\u001b[1mStandardizer @ 5…72\u001b[22m, \u001b[0m\u001b[1mX\u001b[22m)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@constant stand = trainable(stand_model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a `TrainableModel` object, `stand`, which will store the means and standard deviations of the training data columns. Transformed data can be fetched from a new node we will call `Xstand`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transform(\u001b[32m\u001b[1mstand\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@constant Xstand = transform(stand, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all nodes, `Xstand` is a callable object. To see the outcome of our rescaling on the training data, we call `Xstand` with no arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "\u001b[0m\u001b[1mTrainableModel @ stand\u001b[22m with model \u001b[0m\u001b[1mStandardizer @ 5…72\u001b[22m is not trained and so cannot transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0m\u001b[1mTrainableModel @ stand\u001b[22m with model \u001b[0m\u001b[1mStandardizer @ 5…72\u001b[22m is not trained and so cannot transform.",
      "",
      "Stacktrace:",
      " [1] error(::String) at ./error.jl:33",
      " [2] transform(::MLJ.TrainableModel{Standardizer}, ::DataFrame) at /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:129",
      " [3] (::MLJ.LearningNode{MLJ.TrainableModel{Standardizer}})() at /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:215",
      " [4] top-level scope at In[7]:1"
     ]
    }
   ],
   "source": [
    "Xstand() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! To fetch this data we will need to train the node first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ stand\u001b[22m whose model is \u001b[0m\u001b[1mStandardizer @ 5…72\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Crim</th><th>Zn</th><th>Indus</th><th>NOx</th><th>Rm</th><th>Age</th><th>Dis</th><th>Rad</th><th>Tax</th><th>PTRatio</th><th>Black</th><th>LStat</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><tr><th>1</th><td>-0.613361</td><td>0.0849045</td><td>-0.996041</td><td>0.274966</td><td>0.253795</td><td>0.163317</td><td>-0.188693</td><td>-2.1573</td><td>-0.214089</td><td>-1.10018</td><td>0.406865</td><td>-0.91672</td></tr><tr><th>2</th><td>-0.580553</td><td>-0.606926</td><td>-0.214888</td><td>-0.399953</td><td>0.0254423</td><td>0.645338</td><td>0.23768</td><td>-1.53592</td><td>-1.00783</td><td>0.0415407</td><td>0.406865</td><td>-0.209159</td></tr><tr><th>3</th><td>-0.580584</td><td>-0.606926</td><td>-0.214888</td><td>-0.399953</td><td>1.15831</td><td>0.019063</td><td>0.23768</td><td>-1.53592</td><td>-1.00783</td><td>0.0415407</td><td>0.305854</td><td>-1.0783</td></tr><tr><th>4</th><td>-0.572644</td><td>-0.606926</td><td>-1.01738</td><td>-0.507548</td><td>0.881022</td><td>-0.519252</td><td>0.770027</td><td>-0.914528</td><td>-1.30181</td><td>0.452562</td><td>0.350527</td><td>-1.2637</td></tr><tr><th>5</th><td>-0.515311</td><td>-0.606926</td><td>-1.01738</td><td>-0.507548</td><td>1.10196</td><td>-0.223706</td><td>0.770027</td><td>-0.914528</td><td>-1.30181</td><td>0.452562</td><td>0.406865</td><td>-0.85719</td></tr><tr><th>6</th><td>-0.576583</td><td>-0.606926</td><td>-1.01738</td><td>-0.507548</td><td>0.0387876</td><td>-0.0653786</td><td>0.770027</td><td>-0.914528</td><td>-1.30181</td><td>0.452562</td><td>0.33787</td><td>-0.8776</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×12 DataFrame. Omitted printing of 6 columns\n",
       "│ Row │ Crim      │ Zn        │ Indus     │ NOx       │ Rm        │ Age        │\n",
       "│     │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────────┤\n",
       "│ 1   │ -0.613361 │ 0.0849045 │ -0.996041 │ 0.274966  │ 0.253795  │ 0.163317   │\n",
       "│ 2   │ -0.580553 │ -0.606926 │ -0.214888 │ -0.399953 │ 0.0254423 │ 0.645338   │\n",
       "│ 3   │ -0.580584 │ -0.606926 │ -0.214888 │ -0.399953 │ 1.15831   │ 0.019063   │\n",
       "│ 4   │ -0.572644 │ -0.606926 │ -1.01738  │ -0.507548 │ 0.881022  │ -0.519252  │\n",
       "│ 5   │ -0.515311 │ -0.606926 │ -1.01738  │ -0.507548 │ 1.10196   │ -0.223706  │\n",
       "│ 6   │ -0.576583 │ -0.606926 │ -1.01738  │ -0.507548 │ 0.0387876 │ -0.0653786 │"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Xstand)\n",
    "@assert std(Xstand()[:Age]) ≈ 1\n",
    "Xstand() |> head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that fitting the node triggered training of the `TrainableModel` object `stand`.\n",
    "\n",
    "Unfortunately , the K-nearest neighbor model expects `Array` type training data, so we create a new node `Xarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(transform(\u001b[32m\u001b[1mstand\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@constant Xarray = array(Xstand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that implicit in the definition of `Xarray` is the entire network, beginning at the source `X`, which we can see from the above REPL output.\n",
    "\n",
    "Now our K-nearest neighbor model will look to `Xarray` and `y` to fetch its training data, so we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainable(\u001b[0m\u001b[1mKNNRegressor @ 1…54\u001b[22m, \u001b[0m\u001b[1mXarray\u001b[22m, \u001b[0m\u001b[1my\u001b[22m)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@constant knn = trainable(KNNRegressor(K=4), Xarray, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new node where predictions of the KNN model may be fetched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict(\u001b[32m\u001b[1mknn\u001b[22m\u001b[39m, array(transform(\u001b[32m\u001b[1mstand\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = predict(knn, Xarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, fitting the new node causes all dependent `TrainableModel` objects to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ stand\u001b[22m whose model is \u001b[0m\u001b[1mStandardizer @ 5…72\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n",
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ knn\u001b[22m whose model is \u001b[0m\u001b[1mKNNRegressor @ 1…54\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predict(\u001b[32m\u001b[1mknn\u001b[22m\u001b[39m, array(transform(\u001b[32m\u001b[1mstand\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch an actual prediction *on new data* we call our node with the new data as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.675912148491223"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms(ytest, yhat(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we wish to retrain `knn` with a new hyperparameter without bothering to refit our scaling transformer, we can \"freeze\" the scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict(\u001b[32m\u001b[1mknn\u001b[22m\u001b[39m, array(transform(\u001b[31m\u001b[1mstand\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freeze!(stand)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `stand` appears in red, instead of green, to indicate that it is frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: \u001b[0m\u001b[1mTrainableModel @ stand\u001b[22m with model \u001b[0m\u001b[1mStandardizer @ 5…72\u001b[22m not trained as it is frozen.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:84\n",
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ knn\u001b[22m whose model is \u001b[0m\u001b[1mKNNRegressor @ 1…54\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.32782969364479"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.model.K = 7\n",
    "fit!(yhat)\n",
    "rms(ytest, yhat(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Wrapping the network as a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Standardizer` and `KNNRegressor` are examples of MLJ *model* types. While a model is just a container for the hyperparameters of some learning algorithm, every such model has a low-level `fit` and `update` method which implements the kind of training observed above. (The complete specification for these methods is given [here](https://github.com/alan-turing-institute/MLJ.jl/blob/master/doc/adding_new_models.md).) Additionally, there is a low-level `transform` method dispatching on `Standardizer` objects, and a low-level `predict` method dispatching on `KNNRegressor` objects. \n",
    "\n",
    "To bundle the learning network defined above as a stand-alone \"composite\" model, we define a new model type and implement corresponding `fit`, `update` and `predict` methods. We can give our composite model hyperparameters to control exacly how how retraining should look; for example, fit the transformer only once, or arrange for retraining only when relevant hyperparameters change. The example below implements the latter design.\n",
    "\n",
    "First, we define a new model, the container for the composite model's hyperparameters, which in this case are other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MLJ: MLJType, Supervised, LearningNode, fit, update, predict\n",
    "\n",
    "mutable struct WrappedKNN <: Supervised{LearningNode}\n",
    "    stand_model::Standardizer\n",
    "    knn_model::KNNRegressor\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a struct to remember details of the learning network we will construct in calls to `fit`. This is needed for the `update` method, which contains the retraining logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Cache <: MLJType\n",
    "    stand\n",
    "    knn\n",
    "    stand_model\n",
    "    knn_model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method simply wraps the code we already wrote above into a function. Additionally it outputs to `cache` the two `TrainableModel`s, so that they can be frozen or reactivated in `update` (which is called with `cache` as an argument) according to whether or not the component models have changed. So that `update` can detect the change, `cache` also contains the model values used in the initial `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit (generic function with 7 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit(composite::WrappedKNN, verbosity, Xtrain, ytrain)\n",
    "\n",
    "    stand_model = composite.stand_model\n",
    "    knn_model = composite.knn_model\n",
    "    \n",
    "    X = node(Xtrain) # instantiates a source node\n",
    "    y = node(ytrain)\n",
    "    \n",
    "    stand = trainable(stand_model, X)\n",
    "    \n",
    "    Xstand = transform(stand, X)\n",
    "    Xarray = array(Xstand)\n",
    "    \n",
    "    knn = trainable(knn_model, Xarray, y)\n",
    "    \n",
    "    yhat = predict(knn, Xarray)\n",
    "\n",
    "    fit!(yhat, verbosity)\n",
    "    \n",
    "    fitresult = yhat\n",
    "    report = knn.report\n",
    "    cache = Cache(stand, knn, deepcopy(stand_model), deepcopy(knn_model))\n",
    "\n",
    "    return fitresult, cache, report\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method just calls the last node of our network on the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 5 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(composite::WrappedKNN, fitresult, Xnew) = fitresult(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update (generic function with 2 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update(composite::WrappedKNN, verbosity, fitresult, cache, X, y)\n",
    "\n",
    "    stand, knn = cache.stand, cache.knn\n",
    "    stand_model, knn_model = cache.stand_model, cache.knn_model\n",
    "\n",
    "    case1 = (composite.stand_model == stand_model) # true if `stand_model` has not changed\n",
    "    case2 = (composite.knn_model == knn_model) # true if `knn_model` has not changed\n",
    "\n",
    "    # we initially activate all trainable models, but leave them in the\n",
    "    # state needed for this call to update (for post-train inspection):\n",
    "    thaw!(stand); thaw!(knn)\n",
    "    \n",
    "    if case1\n",
    "        freeze!(stand)\n",
    "    end\n",
    "    if case1 && case2 \n",
    "        freeze!(knn)\n",
    "    end\n",
    "\n",
    "    fit!(fitresult, verbosity)\n",
    "\n",
    "    cache.stand_model = deepcopy(composite.stand_model)\n",
    "    cache.knn_model = deepcopy(composite.knn_model)\n",
    "\n",
    "    return fitresult, cache, knn.report\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to build a new simplified network with just one non-source node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mWrappedKNN @ 5…77\u001b[22m: \n",
       "stand_model             =>   \u001b[0m\u001b[1mStandardizer @ 1…70\u001b[22m\n",
       "knn_model               =>   \u001b[0m\u001b[1mKNNRegressor @ 1…99\u001b[22m\n",
       "\n",
       "## \u001b[0m\u001b[1mStandardizer @ 1…70\u001b[22m: \n",
       "features                =>   0-element Array{Symbol,1}\n",
       "\n",
       "## \u001b[0m\u001b[1mKNNRegressor @ 1…99\u001b[22m: \n",
       "K                       =>   4\n",
       "metric                  =>   euclidean (generic function with 1 method)\n",
       "kernel                  =>   reciprocal (generic function with 1 method)\n",
       "\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite_model = WrappedKNN(Standardizer(), KNNRegressor(K=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainable(\u001b[0m\u001b[1mWrappedKNN @ 5…77\u001b[22m, \u001b[0m\u001b[1mX\u001b[22m, \u001b[0m\u001b[1my\u001b[22m)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@constant composite = trainable(composite_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ composite\u001b[22m whose model is \u001b[0m\u001b[1mWrappedKNN @ 5…77\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n",
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ 1…51\u001b[22m whose model is \u001b[0m\u001b[1mStandardizer @ 1…70\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n",
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ 5…90\u001b[22m whose model is \u001b[0m\u001b[1mKNNRegressor @ 1…99\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predict(\u001b[32m\u001b[1mcomposite\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhat = predict(composite, X)\n",
    "fit!(zhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the stadardization hyperparameter triggers retraining of all components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ composite\u001b[22m whose model is \u001b[0m\u001b[1mWrappedKNN @ 5…77\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n",
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ 1…51\u001b[22m whose model is \u001b[0m\u001b[1mStandardizer @ 1…70\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n",
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ 5…90\u001b[22m whose model is \u001b[0m\u001b[1mKNNRegressor @ 1…99\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predict(\u001b[32m\u001b[1mcomposite\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite.model.stand_model.features = [:Age, :Crim, :Zn]\n",
    "fit!(zhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, changing only a KNN hyperparameter does not trigger retraining of the standardizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ composite\u001b[22m whose model is \u001b[0m\u001b[1mWrappedKNN @ 5…77\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n",
      "┌ Warning: \u001b[0m\u001b[1mTrainableModel @ 1…51\u001b[22m with model \u001b[0m\u001b[1mStandardizer @ 1…70\u001b[22m not trained as it is frozen.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:84\n",
      "┌ Info: Training \u001b[0m\u001b[1mTrainableModel @ 5…90\u001b[22m whose model is \u001b[0m\u001b[1mKNNRegressor @ 1…99\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predict(\u001b[32m\u001b[1mcomposite\u001b[22m\u001b[39m, \u001b[0m\u001b[1mX\u001b[22m)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite.model.knn_model.K = 3\n",
    "fit!(zhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
